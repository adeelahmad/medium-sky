e4cef79dc831	"KEYWORDS=dagster, airflow, data orchestration, developer productivity, asset-based approach, container native orchestration, data passing, event-driven execution, backfills, local development.

SUMMARY: This article discusses how data practitioners can choose between Dagster and Apache Airflow, two popular data orchestration tools. Dagster takes a radically different asset-based approach to orchestrating data compared to Airflow's task-based approach. Dagster is designed to make data teams more productive, with built-in tools for local development, testing, and debugging, as well as a declarative asset model for orchestrating data pipelines. The article highlights differences between the two tools in terms of their architecture, approach to data passing, event-driven execution, backfills, local development, and deployment management. Dagster offers a container native approach to orchestration and provides an asset-focused methodology for building and operating data pipelines, while Airflow's architecture is task-based and emphasizes imperative workflows. The article explains how Dagster's asset-centric approach makes it easier to integrate with other modern data stack tools, like DBT, Fivetran, and Airbyte, and offers a unified system for tracking dependencies at the asset level. In conclusion, Dagster is ideal for data teams looking to build and operate data assets, while Airflow is better suited for simple task-based workflows."
6bad1b1dbc36	"KEYWORDS=Python, structuring projects, modularity, best practices, naming conventions, version control, automated testing, dependency management, project folder structure, shared code organization. 

SUMMARY=This blog article, which is part three of a series, offers guidelines to data engineers new to Python on how to structure Python projects for optimal maintainability and scalability. It recommends modularity, naming conventions, version control, automated testing, dependency management, and shared code organization as best practices, and explains how to use a consistent folder structure to keep projects organized. The article also provides a demo of a data engineering project that incorporates these best practices, and highlights key files and subfolders typically found in a Python project."
26d3e97b8c11	"KEYWORDS=faster deploy, warm docker containers, serverless development, feedback loops, dagster cloud, remote environments, docker images, github action, dagster code deployment, pex, repeatability.

SUMMARY=This blog post by Shalabh Chaturvedi describes how they moved from using standard Docker images and builds for Dagster Cloud Serverless development, to a new fast deploy capability called pex. The post outlines the problems with using Docker images and how they analyzed and implemented a system for shipping code outside Docker images. Pex files are then introduced as an alternative to Docker images for repeatable and consistent environments. Using Pex files is faster and has increased deployment speed significantly compared to deployment time with Docker images. The article concludes with some additional optimizations made to the process and the benefits of using Pex files and their combination with Docker images.

"
e26681418e09	"KEYWORDS=fake GitHub stars, black market, Dagster, social proof, vanity metric, machine learning, spam detection, clustering, precision, recall.

SUMMARY: The blog post discusses the issue of fake GitHub stars and the black market around them, as they are considered a vanity metric and an indicator of social proof on the platform. The author shares their approach to identifying fake stars using the GitHub API, pyGithub, and unsupervised clustering techniques. They also mention the limitations of machine learning-based approaches due to spammers actively avoiding detection, and the importance of simple heuristics and human analysis in detecting fake accounts. The post includes an example case of a suspicious repository with a large number of suspected fake stars identified using these methods. Finally, the author acknowledges that the problem of fake accounts is continuously evolving, and there is a need to constantly adapt and refine detection methods."
e4aaf579cc8f	"SUMMARY: The article discusses the importance of partitioned data pipelines in building and operating efficient and cost-effective data pipelines. Partitioning helps to organize data computations and makes data pipelines performant and cost-efficient by letting operators operate on subsets of data. The article explains that partitioning also makes it easy to visualize and operate on bulk data, and that partitioned data pipelines naturally form a collection of miniature data pipelines. The article distinguishes between orchestrators like Airflow and data modeling tools like Dagster, which is capable of modeling partitioned data assets, as well as the partitioned dependencies between them. 

KEYWORDS: data pipelines, partitioning, performance, cost efficiency, data assets, orchestration, partitions, Dagster, Airflow, dependency, backfills"
e2fe65528a71	"SUMMARY: This text is a primer on Python packages and dependency management for data engineers and developers. It covers the basics of packages, modules, and imports, as well as how to manage dependencies using tools like pip and virtual environments. The article also compares the old way of managing dependencies through setup files to the newer approach using the Pyproject.toml file and highlights alternative tools like Poetry for dependency management.

KEYWORDS=Python packages, data engineers, developers, dagster, pip, virtual environments, Pyproject.toml, Poetry, dependency management, modules, imports, setup files, package dependencies."
36f7e601436b	KEYWORDS=migrating, dagster, airflow, data engineering, tooling, orchestration, software-defined assets, migration guide, tutorial, developer ergonomics. SUMMARY: The article discusses the advantages of migrating from Apache Airflow to Dagster, an open-source tool for data engineering teams that provides better orchestration, developer ergonomics and observability. The author provides a migration guide and tutorial, highlighting the benefits of Dagster's software-defined assets (SDAs), which make it easier to manage and execute pipelines. The article also provides examples of organizations that have successfully migrated from Airflow to Dagster, and suggests joining the growing Dagster community for additional support.
856394a05916	"SUMMARY=The Dagster Core team faced a problem: they wanted to skip the right code in GitHub by providing best-class support. To build this support, they wondered if they could create a Slack bot based on technology that could answer basic technical questions. They set out on a journey to figure out if they could use OpenAI's GPT-3 and build a bot that could answer basic technical questions about Dagster. In this text, we read about their process of fine-tuning GPT-3, teaching GPT-3 technical details about Dagster, constructing prompts, utilizing Langchain, creating a search index, and applying Github repo. 

KEYWORDS=Dagster, GitHub, Slack, OpenAI, GPT-3, Langchain, search index, markdown files, vector space search, software-defined assets."
f3a86de9d47b	"SUMMARY: The text explains how to migrate ETL (Extract Transform Load) scripts to a DAGster pipeline using software-defined assets. It discusses the challenges of ETL pipelines, such as complexity, maintenance, scheduling, failures, and testing, and offers solutions to these problems by using DAGster's built-in functionalities like retry policy, freshness policy, and IO manager. The article highlights the benefits of using DAGster, such as modularity, abstraction, and increased productivity, and provides a step-by-step guide on how to migrate ETL scripts to DAGster using Python code examples.

KEYWORDS: ETL script, software-defined assets, DAGster, pipeline, extract, transform, load, Python scripts, dependencies, reliability, testing, modularization, abstraction, IO manager, retries, freshness policy, storage, business logic."
85bd5fa6d707	"SUMMARY: The text describes the concept of declarative asset-based scheduling and how it is used in data pipelines. It introduces Dagster, a data pipelining tool that thinks in terms of materializing data assets and applies freshness policies to keep data up-to-date. The approach employs a graph of data assets, and the system automatically schedules asset materializations to avoid unnecessary computation. The article also includes a discussion of asset freshness policies, granular versioning, and asset reconciliation.

KEYWORDS: declarative scheduling, data assets, Dagster, freshness policies, versioning, materialization, asset reconciliation, data pipelines, upstream data changes, orchestrators."
b9f89c0c2921	"SUMMARY=This article talks about ways to improve the output of software engineers by making them productive. The author shares tips on how to make code changes that increase utility and are reviewed by peers to ensure functional high-quality code. They suggest that feedback loops are key to becoming more productive and offer ways to optimize the human loop in the code review process. The concept of stacked diffs or Pull Requests is introduced, which helps in breaking down large changes into manageable pieces, sending them for review and reworking them incrementally. The author also emphasizes the importance of code review as a chance to cooperate with others in pursuit of better software. 

KEYWORDS=productive, software engineering, code change, utility, feedback loops, peers, review, quality, stacked diffs, Pull Requests, optimization, human loop, bug fix, improvement, documentation, new feature."
ef25ad796d1e	"SUMMARY: The text discusses the importance of type annotations and type checking in large Python codebases, especially in dynamically typed languages like Python. It mentions the benefits of using tools like MyPy and Pyright, and the process of adding type annotations to a large codebase like Dagster. The text also discusses issues related to Python's public API and the importance of marking packages as typed. It ends by highlighting the importance of continuous integration and completeness checks in ensuring type correctness in a large codebase.

KEYWORDS=Python, type annotations, type checking, MyPy, Pyright, dynamically typed, scalability, refactoring, automated tests, JavaScript, TypeScript, development, public API, Dagster, type coverage, Sphinx extension, integration libraries, completeness checks."
3d910a7d593e	"SUMMARY: This text discusses the benefits of using the open-source data orchestrator Dagster to build and manage machine learning pipelines. The article highlights the challenges of creating and maintaining data pipelines for model training and evaluation, and how Dagster can help address these issues by providing a lightweight, Pythonic API that allows for easy iteration and experimentation. With its ability to manage both data and execution dependencies in a single asset graph, Dagster streamlines the pipeline development process and facilitates understanding of how changes impact models and production results. 

KEYWORDS: Dagster, machine learning, pipelines, data orchestrator, asset graph, model training, model evaluation, Pythonic API, experimentation, data dependencies, execution dependencies, production, reliability, observability, integration, Jupyter notebooks."
3154b4b2baef	"SUMMARY: The article discusses the challenges associated with handling large volumes of genetic and clinical data in precision medicine and drug discovery. It showcases how the Washington-based company, Zephyr, uses the Dagster platform to ingests, manage, and process complex, disparate molecular, experimental, and clinical data of patients. The team relies on domain expertise, data scientists, machine learning algorithms, and predictive analytics to create new insights in fields like cancer and chronic disease progression. The article also highlights Dagster's benefits, including shared logging, observability, parameterization functions, and centralization control plane. Teams working on different datasets and using different tools can collaborate efficiently without stepping on each other's toes.

KEYWORDS= Zephyr, Dagster, data science, personalized medicine, genetic dataset, clinical data, predictive analytics, machine learning algorithms, cancer, diabetes."
b73c85e9658f	"SUMMARY=This is a crash course post from the Dagster blog, which is created by Elementl company. Dagster is a data orchestrator framework used to build data pipelines, similar to Django for building web apps. Dagster supports data engineers, machine learning engineers, software engineers, and data scientists in building data pipeline platforms. The post provides a quick and realistic example of building an ETL pipeline that pulls data from Github and visualizes it. The post also covers some of Dagster's advanced features, such as using resources to configure various external services and working with runtime configuration parameters. Finally, the post also covers how to run tests using pytest and the mock package.

KEYWORDS=Dagster, data orchestrator, Elementl, data pipelines, data assets, data engineers, machine learning engineers, software engineers, data scientists, Github, ETL pipeline, resources, runtime configuration parameters, tests, pytest, mock package."
e392b27aa083	"SUMMARY=This blog post published in 2022 by Pete Hunt and Sandy Ryza explains how to build a local data lake from scratch using DuckDB - a hot feature-rich SQL engine that could replace many common data warehouses. DuckDB is fast enough to run locally, which helps both developers to run tests and for production to access remote data sets efficiently. The authors also discuss how to combine DuckDB with a small number of technologies like Dagster and Parquet to create a powerful multiplayer data lake to serve the needs of many organizations. They provide Python code examples, including how to use SQL transforms and Pandas on DuckDB, how to read and store data as Parquet files, and how to manage I/O operations with an example data pipeline. Finally, the authors point out that while DuckDB is an experimental and rapidly evolving technology, it offers a breath of fresh air for people feeling overwhelmed by the overly complex data tools currently available.

KEYWORDS=DuckDB, data lake, SQL engine, local development, Python, SQL transforms, Pandas, Parquet, data pipeline, Dagster."
52ac881b4826	"SUMMARY=This blog post discusses the powerful technique of speeding up data pipeline development through the use of data pipeline smoke tests. It suggests that data practitioners waste time writing unit tests that could be caught by smoke tests instead. A data pipeline smoke test automatically runs data transformations on empty synthetic data and verifies that the code transformation follows the rules of the data processing language. The post provides examples of stub code for data transformations and suggests using tools like Pandas, SQL, Spark, Dask, and Dagster to help write data pipeline smoke tests. The benefits of smoke testing include catching bugs in five seconds instead of five minutes, avoiding the accidental breaking of pipelines in production, and providing broad test coverage. 

KEYWORDS=data pipeline, smoke test, unit test, Pandas, SQL, Spark, Dask, Dagster, data transformation, synthetic data."
69bbfaf52fec	"SUMMARY=The article discusses the troubleshooting process undertaken by the Elementl team for their Dagster tool, which provides a framework for building and running data pipelines. The team investigated an overwhelming log stream, which led them to discover a troublesome cursor issue related to their use of GraphQL and websockets. They ultimately solved the problem by restructuring their query and passing a cursor variable, and also learned important lessons about using web workers and profiling tools.

KEYWORDS=Dagster, troubleshooting, PostgreSQL, GraphQL, websockets, cursor, React, onSubscriptionData, Elementl, overwhelmed log stream, web worker behavior, troublesome cursor issue, performance, cloud-hosted version, multi-tenant SaaS, scalability, log viewer, streaming, monitoring, debugging, user experience, web workers, Apollo, useSubscription, PubSub, cursor variable, debounce updates, refactoring, Stack Overflow, open-source project, GitHub."
3bda86d8327	"SUMMARY: The article discusses the use of PostgreSQL as a message queue in the Dagster Cloud system, which orchestrates data pipelines. The author shares the team's decision to build a logging system on top of PostgreSQL, which worked well for their use case. The article also describes how the team tackled issues related to scalability and performance, and how they measured and improved the system over time.

KEYWORDS: PostgreSQL, message queue, Dagster Cloud, data pipelines, logging system, performance, scalability, load testing, rate limiting, failover, Redis, multi-region support."
6c610d551b6b	"SUMMARY: Dagster has released a new version named ""Groove"" with multiple highly anticipated features, including pipeline failure sensors, solid-level retries, and direct invocation of solids and resources. The new version includes asset sensors to launch pipeline runs in response to asset updates and a new `dbt_cli_resource` to make it easier to interface with dbt. The release also includes a new `pipeline_failure_sensor` decorator that allows users to execute arbitrary code when a pipeline fails. Top contributors to the release include jrouly, hug0l1ma, and makotonium.

KEYWORDS: Dagster, Groove, pipeline failure sensors, solid-level retries, direct invocation, assets sensors, dbt_cli_resource, pipeline_failure_sensor, contributors."
4898319fa9b6	"SUMMARY=This text is a community memo from Sandy Ryza published in 2021 about the developer-focused product APIs and user interface. Dagster APIs received a lot of positive feedback from users, but many found the learning curve to be steep. To make Dagster more approachable and smooth, several changes have been implemented to reduce boilerplate and progressively disclose complexity, such as enabling earlier and permissive capabilities to be used, making the Dagster type system available using Python type annotations, and allowing users to define solids without context argument. The changes aim to reduce code footprint, make prototyping and productionization easier, and build confidence in working with data as expected. The changes covered include defining solids, reducing the use of APIs, and making it easier to use solids like vanilla Python functions. Finally, the author mentions some changes made to streamline event metadata and provide sample code. 

KEYWORDS=Dagster, APIs, user interface, approachable, smooth, learning curve, reduce boilerplate, progressive disclosure complexity, Python type annotations, solids, code footprint, prototyping, productionization, event metadata, sample code."
b635b1118594	KEYWORDS=incremental adoption, dagster, mapbox, geodata teams, airflow, data pipelines, dev cycles, cost reduction, agility, productivity. SUMMARY=Mapbox's geodata teams constantly update the maps and data powering their developer data products and services. However, continued development with Airflow turned out to be painful and costly. To solve this, they started using Dagster, a solution that allows incremental adoption on top of an existing airflow installation. Dagster lets the team write pipelines using a clean set of abstractions, built locally test development, and compile Dagster pipelines to Airflow dags deployed to existing scheduled instances. The use of Dagster has reduced development cycles and cost, improved agility, and increased productivity for Mapbox's data teams.
e5d297447189	KEYWORDS=airflow, dagster, data orchestration, workflow engine, dependency graphs, testing, metadata, Python, infrastructure, scheduling. SUMMARY: The text compares two data orchestration tools, Airflow and Dagster. While Airflow focuses more on scheduling and monitoring, Dagster offers a complete engineering life cycle and ensures faster development and feedback loops. Dagster eliminates an entire class of errors due to mismatches in execution dependencies and implicit data dependencies, thus making it possible to run computations without infrastructure requirements. Dagster also offers more control over execution in different environments and allows for flexible and customizable scheduling beyond Cron expressions. The tool provides rich metadata and parameterizable functions called solids connected via gradually typed data dependencies. The article concludes by emphasizing the need for a reliable, scalable, and multi-tenant platform for data practitioners dealing with complex data dependencies and workflows.
95b0b5d8477d	"KEYWORDS=Dagster, Lucky Star, Release, Dynamic Orchestration, Backfills, Asset Lineage, Dagster API, Asset Catalog, Python, DAGit
SUMMARY: Dagster has published a new release in 2021, named Lucky Star, which includes dynamic orchestration features such as pipeline subsets execution and processing datasets with underlying data change. The Dagster API supports the new asset catalog with improved interoperability and the Dagster type system with better documentation. There is also an enhanced DAGit with a dedicated backfill interface and improved asset catalog. Additionally, Dagster has introduced the experimental support for dynamic orchestration mapping to facilitate massively parallel computation with specialized distributed computation frameworks. The new documentation site provides comprehensive overviews of the main Dagster concepts and APIs, deployment, integrations, and examples of patterns to build Dagster projects. The community is encouraged to give feedback and contribute to the documentation and development of this system."
a3a90bc1fa99	"KEYWORDS=data platform, shared spaces, non-SQL workflows, dagster framework, Snowflake, dbt, abstractions, local development, production, scheduling

SUMMARY: In a blog post published in 2021, Dennis Hume, a member of the data team at Drizly, shared insights into the process of building a shared data platform that supports multiple stakeholders contributing and collaborating together. The non-SQL workflows posed a challenge, and the team found dagster framework to provide a new shared space to support data roles. The team used Snowflake and dbt for standardized SQL transformations and wanted to leverage additional abstractions beyond dag. The deployment process was standardized, and different stages of development were connected using simpler deployment tools. The team aims to enable self-service data platform separation concerns, allowing small core data infrastructure teams to efficiently ship data platforms, and ensure that anyone can contribute an additional pipeline without real coordination overhead. The team is still streamlining processes and looking at consolidating isolated workflows running in one environment using dagster. Additionally, the team is interested in building data teams and is hiring."
ec7a9215b082	"KEYWORDS=dagster, edge glory, operational maturity, reliability, fault tolerance, reconciliation loop, sensors, event-based schedules, kubernetes, asset-based sensors, data mesh

SUMMARY: Dagster has released a new operating system named Edge Glory that focuses on operational maturity, reliability, and expansion of production-grade use cases. The primary features of the new release include fault-tolerant scheduling, run scheduler semantics based on a reconciliation loop, and fault-tolerant sensors. Edge Glory supports sensors for both event-based and time-based schedules, with the managed scheduler process monitoring external states to ensure that scheduled runs align with the actual state of the workflows. The new release proves especially compelling for users who desire to use data frames as a target for data warehouses and data lakes. The system applies a powerful mental model based on the reconciliation loop to manage complex schedules and dependencies between pipelines, sensors, and assets. The Edge Glory release comes with an integrated scheduler that runs as a daemon process for reliability and also supports Kubernetes deployments. The system also allows for process-level isolation and supports different persistence models through its extensible asset-based sensor framework."
e015127b396	"KEYWORDS=data platform, dagster, dbt, pipeline, automation, optimization, centrality measures, metapipeline, data management, visualization 
SUMMARY=The article discusses the use of Dagster to manage data platforms and streamline data processes. It highlights the challenges presented by accumulating tech debt and relying on disparate tools with implicit dependencies. The solution adopted by Good Eggs was to use Dagster to orchestrate computations, automate the management of unused DBT mode artifacts and maintain DBT exposures, all while reducing the cognitive load on stakeholders. The article concludes by discussing the use of centrality measures and visualization techniques to identify critical DBT models and optimize their performance."
f975d179e809	"SUMMARY: The article talks about the benefits of using the Dagster platform for data observability and asset cataloging in production environments. The author discusses how Dagster integrates with different tools to provide observability and how it includes a built-in asset catalog where specialized structured metadata can be recorded and viewed longitudinally to track the production of assets. The article also highlights the importance of proactively surfacing the data and figures generated by computations, and how Dagster helps close the loop in the analytics workflow by providing a horizontal view of data assets to analysts. 

KEYWORDS= Dagster platform, data observability, asset catalog, metadata, raw data, DBT, data warehouse, production assets, observability products, computations lineage, analytics workflow."
e6bb0c7b1d86	"SUMMARY=The Dagster blog has recently announced the release of the first-class integration between Dagster and DBT, two complementary data build technologies used by data teams. DBT is used to define interdependent tables within a data warehouse, while Dagster is used to monitor and orchestrate the execution of DBT projects, as well as to track tables produced by DBT. Dagster provides built-in operational data observability capabilities to support a single unified operational view, and it lets users embed DBT into wider orchestration graphs. The blog post extensively highlights the benefits of using Dagster and DBT together, alongside other technologies, and provides an example of a Dagster pipeline orchestrating DBT models with heterogeneous processes. 

KEYWORDS=Dagster, DBT, integration, data build tool, data teams, data warehouse, orchestrate, monitoring, operational data observability, dependency graph, SQL transformations, metadata, open source."
eb3beb9fcea1	"KEYWORDS=Data infrastructure, Dagster, data quality, custom data frame types, structured metadata, data ingest, type checks, Google Sheets, data platform, observability. 
SUMMARY=This article, published on the Dagster blog by David J. Wallace in 2020, discusses the importance of good quality data and the significance of data infrastructure like Dagster to ensure data correctness and reliability. The author introduces the idea of transforming data platforms and suggests that good data can come from ""good eggs"" such as field warehouse managers who use Google Sheets to report staffing and labor metrics. The article explores how Dagster can help manage data ingest from Google Sheets and how custom data frame types and structured metadata can help with data quality checks. The importance of type checks, the ability to surface structured metadata directly in Dagster logs, and the benefits of using custom data types to ensure data correctness are also discussed. The article concludes by emphasizing the importance of reducing downstream data breakage and suggests that Dagster can be a valuable tool for data orchestration and better data recovery."
